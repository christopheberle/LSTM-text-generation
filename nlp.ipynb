{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level text generation with LSTM\n",
    "\n",
    "In this notebook I build an LSTM recurrent neural network for character-level text generation. The network will be trained on H.P. Lovecraft's stories which are stored in the `data/lovecraft.txt` file.\n",
    "\n",
    "The network works as follows. We split the text into a set of input sequences $\\mathcal I$ of length `seq_length` (here we used 40 character) and an expected output (label) which is the `seq_length+1`-th character. Next we assign each character $c$ a unique integer and one-hot encode the sequences and labels. Once encoded we train our neural network which consists of a single LSTM layer with 192 units and an output layer with softmax activation function over the characters $c$. Finally we use the functions `sample` and `gen_text` to sample from the output of our trained neural network given an input sequence and generate new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training text length: 2882630 characters\n",
      "Text contains 62 unique characters\n",
      "960864 training sequences\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"data/lovecraft.txt\", \"r\") as file:\n",
    "  text = file.read().lower()\n",
    "\n",
    "text = text.replace(\"\\n\", \"\") # replace new lines for now\n",
    "print(\"Training text length:\", len(text), \"characters\")\n",
    "\n",
    "chars = sorted(set(text))\n",
    "print(\"Text contains {0} unique characters\".format(len(chars)))\n",
    "char2idx = {u : n for n, u in enumerate(chars)}\n",
    "idx2char = {n : u for n, u in enumerate(chars)}\n",
    "\n",
    "seq_length = 40\n",
    "stride = 3\n",
    "sents = []\n",
    "next_chars = []\n",
    "\n",
    "#split text into sequences\n",
    "for i in range(0, len(text) - seq_length, stride):\n",
    "  sents.append(text[i:i+seq_length])\n",
    "  next_chars.append(text[i+seq_length])\n",
    "\n",
    "print(\"{0} training sequences\".format(len(sents)))\n",
    "\n",
    "# One-hot encode the sequences with shape\n",
    "\n",
    "x = np.zeros((len(sents), seq_length, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sents), len(chars)), dtype=np.bool)\n",
    "for n, sent in enumerate(sents):\n",
    "  for m, char in enumerate(sent):\n",
    "    x[n,m,char2idx[char]] = True\n",
    "  y[n, char2idx[next_chars[n]]] = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Input(shape=(seq_length, len(chars))),\n",
    "  LSTM(192),\n",
    "  Dense(len(chars), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5005/5005 [==============================] - 469s 94ms/step - loss: 2.1418\n",
      "Epoch 2/10\n",
      "5005/5005 [==============================] - 484s 97ms/step - loss: 1.8339\n",
      "Epoch 3/10\n",
      "5005/5005 [==============================] - 502s 100ms/step - loss: 1.7204\n",
      "Epoch 4/10\n",
      "5005/5005 [==============================] - 497s 99ms/step - loss: 1.6432\n",
      "Epoch 5/10\n",
      "5005/5005 [==============================] - 487s 97ms/step - loss: 1.5875\n",
      "Epoch 6/10\n",
      "5005/5005 [==============================] - 482s 96ms/step - loss: 1.5454\n",
      "Epoch 7/10\n",
      "5005/5005 [==============================] - 486s 97ms/step - loss: 1.5119\n",
      "Epoch 8/10\n",
      "5005/5005 [==============================] - 484s 97ms/step - loss: 1.4853\n",
      "Epoch 9/10\n",
      "5005/5005 [==============================] - 417s 83ms/step - loss: 1.4634\n",
      "Epoch 10/10\n",
      "5005/5005 [==============================] - 468s 93ms/step - loss: 1.4451\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7f9832426d90>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 192\n",
    "\n",
    "model.fit(x,y,batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting the model we need to obtain new characters by providing an input sequence. The first step is to sample from the output distribution. Recall that our output layer has a softmax activation function, thus the distribution over the next character $c\\in\\mathcal C$ candidate follows a Gibbs distribution\n",
    "$$\n",
    "\\mathcal P(c|\\mathcal I)=\\frac{e^{-\\beta\\mathcal H}}{\\mathcal Z}\n",
    "$$\n",
    "where\n",
    "* $\\beta=\\frac{1}{T}$ is the inverse temperature\n",
    "* $\\mathcal H=-\\log p(c|\\mathcal I)$ is the negative log-likelihood of a character $c$ given an input sequence $\\mathcal I$.\n",
    "* $\\mathcal Z$ is a normalisation constant\n",
    "\n",
    "Sampling a character from this distribution is done via the `sample` methode which takes the output of our model and the temperature $T$ as inputs.\n",
    "\n",
    "Next we generate a string of characters of length $N$ using the `gen_text` function. This function takes as input and input sequence $\\mathcal I$ of length `seq_length`, an integer `length` defining how many characters should be generated and a temperature $T$ for the sampling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "  preds = np.asarray(preds).astype(\"float64\")\n",
    "  H = - np.log(preds) \n",
    "  gibbs = np.exp(-H / temperature)\n",
    "  Z = np.sum(gibbs)\n",
    "  gibbs = gibbs / Z\n",
    "  p = np.random.multinomial(1, gibbs, 1)\n",
    "  return np.argmax(p)\n",
    "\n",
    "def gen_text(seed, length, temperature=1.0):\n",
    "  out = seed\n",
    "  state = seed\n",
    "  for l in tqdm(range(length)):\n",
    "    x = np.zeros((1, seq_length, len(chars)))\n",
    "    for n, char in enumerate(state):\n",
    "      x[0, n, char2idx[char]] = 1.0\n",
    "    preds = model.predict(x, verbose=False)[0]\n",
    "    next_idx = sample(preds, temperature)\n",
    "    next_char = idx2char[next_idx]\n",
    "    state = state[1:] + next_char\n",
    "    out += next_char\n",
    "\n",
    "  return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample outputs (precalculated using Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed for generating new text was"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'when i drew nigh the nameless city i kne'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:seq_length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some generated texts were"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [when i drew nigh the nameless city i kne]w the subtern and prints to the close of the one which he linker of the grave as the accorning the roof in the currous line of the while and the litter him than a man in the real could not a could be \n",
    "* [when i drew nigh the nameless city i kne]w been and the sound and was the profession to one the floor to see the state of the whilp as the general benew must have been and the street had been the processing the railing mind had to see that t\n",
    "* [when i drew nigh the nameless city i kne]w the becamen seen mountain mind and from any your of the while the sease that the men to alter a pable encoment of the frint great stone on the speck which one the mountains which strange of the dema\n",
    "\n",
    "You can generate your own text using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 35.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "'when i drew nigh the nameless city i knew i whach i wadd the vaultous flowaish fatter necreat. abys effacime almost common. there would out been askemoting the band whisper of the telled on the colounible to which had bree west, but we pure'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = text[0:seq_length]\n",
    "length = 200\n",
    "temperature = 1.0\n",
    "gen_text(seed, length, temperature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('IFT': conda)",
   "name": "python383jvsc74a57bd098ca35e79e75585856e7ae377f1a7cbb33a237829a705b81f9e499062aceea75"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}